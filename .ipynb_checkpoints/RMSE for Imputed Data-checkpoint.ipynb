{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ef834339",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "def return_split_con_cat_column(df_miss):\n",
    "    columns = df_miss.columns\n",
    "    columns_split = [i.split('_') for i in columns]\n",
    "    con = []\n",
    "    cat = []\n",
    "    for index, i in enumerate(columns_split):\n",
    "        if i[0][0:3] == 'con':\n",
    "            con.append(columns[index])\n",
    "        elif i[0][0:3] == 'cat':\n",
    "            cat.append(columns[index])\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    return con, cat\n",
    "\n",
    "def return_dictionary_column(column_unique):\n",
    "    dict_res = {}\n",
    "    dict_rev = {}\n",
    "    for index, i in enumerate(column_unique):\n",
    "        dict_res[i] = index\n",
    "        dict_rev[index] = i\n",
    "    \n",
    "    return dict_res, dict_rev \n",
    "\n",
    "def return_encode_column(column_m, dictionary, mode):\n",
    "    column_encode = []\n",
    "    \n",
    "    if mode == 'one_hot':\n",
    "        for index, i in enumerate(column_m):\n",
    "            if i == np.nan or str(i) == 'nan':\n",
    "                column_encode.append([np.nan for j in range(len(dictionary))])\n",
    "            else:\n",
    "                column_encode.append(list(np.eye(len(dictionary))[dictionary[i]]))\n",
    "                \n",
    "        array_encode = np.array(column_encode, dtype = np.float32)\n",
    "    \n",
    "    elif mode == 'embedding':\n",
    "        for index, i in enumerate(column_m):\n",
    "            if i == np.nan or str(i) == 'nan':\n",
    "                column_encode.append([np.nan for j in range(len(dictionary))])\n",
    "            else:\n",
    "                column_encode.append(list(np.eye(len(dictionary))[dictionary[i]] * 2. - 1.))\n",
    "                \n",
    "        array_encode = np.array(column_encode, dtype = np.float32)\n",
    "        \n",
    "    return array_encode\n",
    "    \n",
    "    \n",
    "def return_normalized_column(column_m, column_i, mode):\n",
    "    \n",
    "    if mode == 'one_hot':\n",
    "        max_m = np.nanmax(np.array(column_m))\n",
    "        min_m = np.nanmin(np.array(column_m))\n",
    "        max_i = np.max(np.array(column_i))\n",
    "        min_i = np.min(np.array(column_i))\n",
    "        if min_m != max_m:\n",
    "            column_m_normalize = [(i - min_m)/(max_m - min_m) for i in column_m]\n",
    "            column_m_normalize = [i for i in column_m_normalize]\n",
    "        else:\n",
    "            column_m_normalize = [i * 0. for i in column_m]\n",
    "    elif mode == 'embedding':\n",
    "        max_m = np.nanmax(np.array(column_m))\n",
    "        min_m = np.nanmin(np.array(column_m))\n",
    "        max_i = np.max(np.array(column_i))\n",
    "        min_i = np.min(np.array(column_i))\n",
    "        if min_m != max_m:\n",
    "            column_m_normalize = [(i - min_m)/(max_m - min_m) for i in column_m]\n",
    "            column_m_normalize = [i * 2. - 1. for i in column_m_normalize]\n",
    "        else:\n",
    "            column_m_normalize = [0. for i in column_m]\n",
    "    \n",
    "    array_m_normalized = np.array(column_m_normalize, np.float32).reshape(len(column_m_normalize), 1)\n",
    "    \n",
    "    return array_m_normalized, max_m, min_m, max_i, min_i\n",
    "\n",
    "def return_data_miss_and_full_train(miss_method, index_miss, index_file, mode = 'one_hot'):\n",
    "    #print(\"Now, Data Frames for Case {0} with full version, missing {1:.2%} are being prepared...\".format(index_case, float(index_miss/100)))\n",
    "    df_miss = pd.read_csv('data_stored/data_miss/{}/Case14/miss{}/{}.csv'.format(miss_method, index_miss, index_file))\n",
    "    df_full = pd.read_csv('data_stored/data/14.csv')\n",
    "    con, cat = return_split_con_cat_column(df_miss = df_miss)\n",
    "    \n",
    "    #print(con, cat)\n",
    "    # Define the labels, location below:\n",
    "    labels = []\n",
    "    labels_ori = []\n",
    "    locations = []\n",
    "    attach = []\n",
    "    \n",
    "    # Start encode the cat variables:\n",
    "    #print(\"Now, the preprocessing of data have already started, with mode: \" + mode)\n",
    "    for index, i in enumerate(df_full.columns):\n",
    "        if i in cat:\n",
    "            column_m = df_miss[i].to_list()\n",
    "            column_i = df_full[i].to_list()\n",
    "            columns_m_unique = list(set([i for i in column_m if str(i) != 'nan']))\n",
    "            columns_i_unique = list(set([i for i in column_i]))\n",
    "            dict_res_column, dict_rev_column = return_dictionary_column(column_unique = columns_m_unique)\n",
    "            dictori_res_column, dictori_rev_column = return_dictionary_column(column_unique = columns_i_unique)\n",
    "            column_encode = return_encode_column(column_m = column_m, dictionary = dict_res_column, mode = mode)\n",
    "            locations.append(column_encode.shape[1])\n",
    "            labels.append(['cat', [dict_res_column, dict_rev_column]])\n",
    "            labels_ori.append(['cat', [dictori_res_column, dictori_rev_column]])\n",
    "            attach.append(['cat', columns_i_unique]) \n",
    "        elif i in con:\n",
    "            column_m = df_miss[i].to_list()\n",
    "            column_i = df_full[i].to_list()\n",
    "            column_encode, max_m, min_m, max_i, min_i = return_normalized_column(column_m = column_m, column_i = column_i, mode = mode)\n",
    "            locations.append(column_encode.shape[1])\n",
    "            labels.append(['con', [max_m, min_m]])\n",
    "            labels_ori.append(['con', [max_i, min_i]])\n",
    "            attach.append(['con', [max_m, min_m]])\n",
    "            \n",
    "        array_encode_column = np.array(column_encode, np.float32)\n",
    "\n",
    "        if index == 0:\n",
    "            array_result = array_encode_column\n",
    "        else:\n",
    "            array_result = np.concatenate((array_result, array_encode_column), axis = 1)\n",
    "            locations[-1] = locations[-1] + locations[-2]\n",
    "    #print(\"Now, the preprocessing of data had been finished.\\n\")   \n",
    "    return array_result, df_full.columns, locations, labels, df_full, df_miss, attach, labels_ori\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "aca7ae45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def return_encode_column_R(column_m, dictionary):\n",
    "    column_encode = []\n",
    "    for index, i in enumerate(column_m):\n",
    "        if i == np.nan or str(i) == \"nan\":\n",
    "            column_encode.append([np.nan for j in range(len(dictionary))])\n",
    "            print(\"...there is nan in imputed data...\")\n",
    "        else:\n",
    "            column_encode.append(list(np.eye(len(dictionary))[dictionary[i]]))\n",
    "\n",
    "    array_encode = np.array(column_encode, dtype = np.float32)\n",
    "        \n",
    "    return array_encode\n",
    "    \n",
    "    \n",
    "def return_normalized_column_R(column_m):\n",
    "    max_m = np.max(np.array(column_m))\n",
    "    min_m = np.min(np.array(column_m))\n",
    "    if min_m != max_m:\n",
    "        column_m_normalize = [(i - min_m)/(max_m - min_m) for i in column_m]\n",
    "        column_m_normalize = [i for i in column_m_normalize]\n",
    "    else:\n",
    "        column_m_normalize = [i * 0. for i in column_m]\n",
    "    \n",
    "    array_m_normalized = np.array(column_m_normalize, np.float32).reshape(len(column_m_normalize), 1)\n",
    "    \n",
    "    return array_m_normalized, max_m, min_m\n",
    "\n",
    "def return_data_miss_and_full_R(miss_method, index_miss, index_file):\n",
    "    # full imputed data\n",
    "    df_miss = pd.read_csv('data_stored/data_dsan/{}/Case14/miss{}/{}.csv'.format(miss_method, index_miss, index_file))\n",
    "    con, cat = return_split_con_cat_column(df_miss = df_miss)\n",
    "    #print(con, cat)\n",
    "    # Define the labels, location below:\n",
    "    labels = []\n",
    "    locations = []\n",
    "    \n",
    "    # Start encode the cat variables:\n",
    "    #print(\"Now, the preprocessing of data have already started, with mode: \" + mode)\n",
    "    for index, i in enumerate(df_miss.columns):\n",
    "        if i in cat:\n",
    "            column_m = df_miss[i].to_list()\n",
    "            columns_m_unique = list(set([i for i in column_m if str(i) != \"nan\"]))\n",
    "            dict_res_column, dict_rev_column = return_dictionary_column(column_unique = columns_m_unique)\n",
    "            column_encode = return_encode_column_R(column_m = column_m, dictionary = dict_res_column)\n",
    "            locations.append(column_encode.shape[1])\n",
    "            labels.append([\"cat\", [dict_res_column, dict_rev_column]])\n",
    "        elif i in con:\n",
    "            column_m = df_miss[i].to_list()\n",
    "            column_encode, max_m, min_m = return_normalized_column_R(column_m = column_m)\n",
    "            locations.append(column_encode.shape[1])\n",
    "            labels.append([\"con\", [max_m, min_m]])\n",
    "            \n",
    "        array_encode_column = np.array(column_encode, np.float32)\n",
    "\n",
    "        if index == 0:\n",
    "            array_result = array_encode_column\n",
    "        else:\n",
    "            array_result = np.concatenate((array_result, array_encode_column), axis = 1)\n",
    "            locations[-1] = locations[-1] + locations[-2]\n",
    "    #print(\"Now, the preprocessing of data had been finished.\\n\")   \n",
    "    return array_result, df_miss.columns, locations, labels, df_miss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0ff1b8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_data_miss_and_full(miss_method, index_miss, index_file, mode = 'one_hot'):\n",
    "    return return_data_miss_and_full_train(miss_method = miss_method, index_miss = index_miss, index_file = index_file, mode = mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "380cb270",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class Model_test():\n",
    "    def __init__(self, label_reverse, label_ori, column_location, column_name, mode = 'one_hot'):\n",
    "        self.mode = mode\n",
    "        self.label_reverse = label_reverse\n",
    "        self.label_ori = label_ori\n",
    "        self.column_location = column_location\n",
    "        self.column_name = column_name\n",
    "        \n",
    "       \n",
    "    def return_accuary_for_con_cat(self, index, generate_i, mask_i, list_original_label):\n",
    "        if self.label_reverse[index][0] == 'con':\n",
    "            max_ = self.label_reverse[index][1][0]\n",
    "            min_ = self.label_reverse[index][1][1]\n",
    "            max_ori = self.label_ori[index][1][0]\n",
    "            min_ori = self.label_ori[index][1][1]\n",
    "            if self.mode == 'embedding':\n",
    "                generate_i = (generate_i + 1.)/2.\n",
    "            generate_i_re = generate_i * (max_ - min_) + min_\n",
    "            generate_i_ori = (generate_i_re - min_ori)/(max_ori - min_ori)\n",
    "            label_i_ori = np.array(list_original_label, np.float32).reshape(len(list_original_label), 1)\n",
    "            if max_ori != min_ori:\n",
    "                label_i_ori = (label_i_ori - min_ori)/(max_ori - min_ori)\n",
    "            else:\n",
    "                label_i_ori = (label_i_ori - min_ori) * 0.\n",
    "\n",
    "            return ['con', np.sum(((generate_i_ori - label_i_ori)**2) * (1. - mask_i)), np.sum((1. - mask_i))]\n",
    "\n",
    "        else:\n",
    "            dictionary = self.label_reverse[index][1][1]\n",
    "            generate_i_argmax = list(np.argmax(generate_i, axis = 1))\n",
    "            mask_i_argmax = mask_i[:, 0]\n",
    "            label_i_argmax = list_original_label\n",
    "            result = np.array([dictionary[generate_i_argmax[i]] == label_i_argmax[i] for i in range(len(label_i_argmax))], dtype = np.float32)\n",
    "            \n",
    "            return ['cat', np.sum(result * (1. - mask_i_argmax)), np.sum((1. - mask_i_argmax))]\n",
    "        \n",
    "    def return_con_loss_cat_accuary_test_result(self, result):\n",
    "        con_loss = 0\n",
    "        con_mask_sum = 0\n",
    "        cat_accuary = 0\n",
    "        cat_mask_sum = 0\n",
    "        \n",
    "        for index, i in enumerate(result):\n",
    "            if i[0] == 'con':\n",
    "                con_loss = con_loss + i[1]\n",
    "                con_mask_sum = con_mask_sum + i[2]\n",
    "            else:\n",
    "                cat_accuary = cat_accuary + i[1]\n",
    "                cat_mask_sum = cat_mask_sum + i[2]\n",
    "                \n",
    "        if con_mask_sum == 0:\n",
    "            con_mask_sum = con_mask_sum + 1  \n",
    "        if cat_mask_sum == 0:\n",
    "            cat_mask_sum = cat_mask_sum + 1\n",
    "            \n",
    "        return float(np.sqrt(con_loss/con_mask_sum)), float(cat_accuary/cat_mask_sum)\n",
    "        \n",
    "    def cross_validation_result(self, data_list, mask_list, df_full_list, df_miss_list):\n",
    "        for index, i in enumerate(data_list):\n",
    "            if index == 0:\n",
    "                data = i\n",
    "                mask = mask_list[index]\n",
    "            else:\n",
    "                data = np.concatenate((data, i), axis = 0)\n",
    "                mask = np.concatenate((mask, mask_list[index]), axis = 0)\n",
    "                \n",
    "        df_full = pd.concat(df_full_list)\n",
    "        df_miss = pd.concat(df_miss_list)\n",
    "        return data, mask, df_full, df_miss\n",
    "        \n",
    "    def model_test(self, data = None, mask = None, df_original = None):\n",
    "        result = []\n",
    "        \n",
    "        for index, i in enumerate(self.column_name):\n",
    "            if index == 0:\n",
    "                generate_i = data[:, 0:self.column_location[index]]\n",
    "                mask_i = mask[:, 0:self.column_location[index]]\n",
    "                list_original_label = df_original[i].to_list()\n",
    "                re = self.return_accuary_for_con_cat(index = index, generate_i = generate_i, mask_i = mask_i, list_original_label = list_original_label)\n",
    "                result.append(re)\n",
    "\n",
    "            else:\n",
    "                generate_i = data[:, self.column_location[index - 1]:self.column_location[index]]\n",
    "                mask_i = mask[:, self.column_location[index - 1]:self.column_location[index]]\n",
    "                list_original_label = df_original[i].to_list()\n",
    "                re = self.return_accuary_for_con_cat(index = index, generate_i = generate_i, mask_i = mask_i, list_original_label = list_original_label)\n",
    "                result.append(re)\n",
    "\n",
    "        con_loss, cat_accuray = self.return_con_loss_cat_accuary_test_result(result = result)\n",
    "        return con_loss, cat_accuray\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c976f45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def returnMedianrange(con_loss_last_array, cat_accuracy_last_array, num_of_test, miss_method, index_miss, name):\n",
    "    \n",
    "    con_loss_last_array = [i for i in con_loss_last_array if str(i) != 'nan']\n",
    "    cat_accuracy_last_array = [i for i in cat_accuracy_last_array if str(i) != 'nan']\n",
    "    \n",
    "    main_path = os.path.join(os.getcwd(), 'performance')\n",
    "    if not os.path.exists(main_path):\n",
    "        os.mkdir(main_path)\n",
    "    method_path = os.path.join(main_path, miss_method)\n",
    "    if not os.path.exists(method_path):\n",
    "        os.mkdir(method_path)\n",
    "    miss_case = os.path.join(method_path, 'Case14')\n",
    "    if not os.path.exists(miss_case):\n",
    "        os.mkdir(miss_case)\n",
    "    miss_path = os.path.join(miss_case, 'miss{0}'.format(index_miss))\n",
    "    if not os.path.exists(miss_path):\n",
    "        os.mkdir(miss_path)\n",
    "    perform_path = os.path.join(miss_path, name + '_performance.csv')\n",
    "    perform_summary_path = os.path.join(miss_path, name + '_summary.csv')\n",
    "    \n",
    "    #df_res = pd.DataFrame()\n",
    "    #df_res['con_loss'] = con_loss_last_array\n",
    "    #df_res['cat_accuracy'] = cat_accuracy_last_array\n",
    "    #df_res.to_csv(perform_path, index = False)\n",
    "\n",
    "    con_loss_last_array.sort()\n",
    "    cat_accuracy_last_array.sort()\n",
    "    num_of_test = len(con_loss_last_array)\n",
    "    medCon = np.median(con_loss_last_array)\n",
    "    medCat = np.median(cat_accuracy_last_array)\n",
    "    q1Con = np.median(con_loss_last_array[:int(num_of_test/2)])\n",
    "    q2Con = np.median(con_loss_last_array[int(num_of_test/2):])\n",
    "    q1Cat = np.median(cat_accuracy_last_array[:int(num_of_test/2)])\n",
    "    q2Cat = np.median(cat_accuracy_last_array[int(num_of_test/2):])\n",
    "    \n",
    "\n",
    "    meanCon = np.mean(con_loss_last_array)\n",
    "    varCon = np.std(con_loss_last_array)\n",
    "    meanCat = np.mean(cat_accuracy_last_array)\n",
    "    varCat = np.std(cat_accuracy_last_array)\n",
    "\n",
    "    df_rem = pd.DataFrame()\n",
    "    df_rem['medCon'] = [medCon]\n",
    "    df_rem['qRCon'] = [q2Con - q1Con]\n",
    "    df_rem['medCat'] = [medCat]\n",
    "    df_rem['qRCat'] = [q2Cat - q1Cat]\n",
    "\n",
    "    df_rem['meanCon'] = [np.around(meanCon, 4)]\n",
    "    df_rem['varCon'] = [np.around(varCon, 4)]\n",
    "    df_rem['meanCat'] = [np.around(meanCat, 4)]\n",
    "    df_rem['varCat'] = [np.around(varCat, 4)]\n",
    "    df_rem.to_csv(perform_summary_path, index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "22da6cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_mask_of_data(data):\n",
    "    mask = 1. - np.isnan(data)   \n",
    "    return mask\n",
    "  \n",
    "def Rimputed_test(miss_method, index_miss, imputed_method = 'MissForest_py', num_of_test = 100):\n",
    "    con_res = []\n",
    "    cat_res = []\n",
    "    for index in tqdm(range(num_of_test)):\n",
    "        try:\n",
    "            data, _, _, _, df_full, _, _, labels_ori = return_data_miss_and_full(miss_method = miss_method, index_miss = index_miss, index_file = index, mode = 'one_hot')\n",
    "\n",
    "            mask = return_mask_of_data(data = data)\n",
    "            #print(\"Now, Data Frames for Case {0} with full version, missing {1:.2%} are being prepared...\".format(index_case, float(index_miss/100)))\n",
    "            data_imputed, column_name, column_location, labels, _ = return_data_miss_and_full_R(miss_method = miss_method, index_miss = index_miss, index_file = index)\n",
    "\n",
    "            mt = Model_test(label_reverse = labels, label_ori = labels_ori, column_location = column_location, column_name = column_name)\n",
    "            con_loss, cat_acc = mt.model_test(data = data_imputed, mask = mask, df_original = df_full)\n",
    "            con_res.append(con_loss)\n",
    "            cat_res.append(cat_acc)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    #print(\"Now, the preprocessing of data had been finished.\\n\")\n",
    "    returnMedianrange(con_loss_last_array = con_res, cat_accuracy_last_array = cat_res, num_of_test = num_of_test, miss_method = miss_method, index_miss = index_miss, name = imputed_method)\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "558dd91f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                         | 0/3 [00:00<?, ?it/s]\n",
      "  0%|                                                        | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|████▊                                           | 1/10 [00:00<00:03,  2.62it/s]\u001b[A\n",
      " 20%|█████████▌                                      | 2/10 [00:00<00:02,  3.07it/s]\u001b[A\n",
      " 30%|██████████████▍                                 | 3/10 [00:00<00:02,  3.13it/s]\u001b[A\n",
      " 40%|███████████████████▏                            | 4/10 [00:01<00:01,  3.04it/s]\u001b[A\n",
      " 50%|████████████████████████                        | 5/10 [00:01<00:01,  3.09it/s]\u001b[A\n",
      " 60%|████████████████████████████▊                   | 6/10 [00:01<00:01,  3.13it/s]\u001b[A\n",
      " 70%|█████████████████████████████████▌              | 7/10 [00:02<00:00,  3.16it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...there is nan in imputed data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 80%|██████████████████████████████████████▍         | 8/10 [00:02<00:00,  3.17it/s]\u001b[A\n",
      " 90%|███████████████████████████████████████████▏    | 9/10 [00:02<00:00,  3.18it/s]\u001b[A\n",
      "100%|███████████████████████████████████████████████| 10/10 [00:03<00:00,  3.12it/s]\u001b[A\n",
      " 33%|████████████████▎                                | 1/3 [00:03<00:06,  3.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...there is nan in imputed data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                        | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|████▊                                           | 1/10 [00:00<00:02,  3.63it/s]\u001b[A\n",
      " 20%|█████████▌                                      | 2/10 [00:00<00:02,  3.46it/s]\u001b[A\n",
      " 30%|██████████████▍                                 | 3/10 [00:00<00:02,  3.40it/s]\u001b[A\n",
      " 40%|███████████████████▏                            | 4/10 [00:01<00:01,  3.31it/s]\u001b[A\n",
      " 50%|████████████████████████                        | 5/10 [00:01<00:01,  3.33it/s]\u001b[A\n",
      " 60%|████████████████████████████▊                   | 6/10 [00:01<00:01,  3.32it/s]\u001b[A\n",
      " 70%|█████████████████████████████████▌              | 7/10 [00:02<00:00,  3.33it/s]\u001b[A\n",
      " 80%|██████████████████████████████████████▍         | 8/10 [00:02<00:00,  3.34it/s]\u001b[A\n",
      " 90%|███████████████████████████████████████████▏    | 9/10 [00:02<00:00,  3.36it/s]\u001b[A\n",
      "100%|███████████████████████████████████████████████| 10/10 [00:02<00:00,  3.36it/s]\u001b[A\n",
      " 67%|████████████████████████████████▋                | 2/3 [00:06<00:03,  3.11s/it]\n",
      "  0%|                                                        | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|████▊                                           | 1/10 [00:00<00:02,  3.86it/s]\u001b[A\n",
      " 20%|█████████▌                                      | 2/10 [00:00<00:02,  3.59it/s]\u001b[A\n",
      " 30%|██████████████▍                                 | 3/10 [00:00<00:01,  3.55it/s]\u001b[A\n",
      " 40%|███████████████████▏                            | 4/10 [00:01<00:01,  3.54it/s]\u001b[A\n",
      " 50%|████████████████████████                        | 5/10 [00:01<00:01,  3.55it/s]\u001b[A\n",
      " 60%|████████████████████████████▊                   | 6/10 [00:01<00:01,  3.53it/s]\u001b[A\n",
      " 70%|█████████████████████████████████▌              | 7/10 [00:01<00:00,  3.52it/s]\u001b[A\n",
      " 80%|██████████████████████████████████████▍         | 8/10 [00:02<00:00,  3.49it/s]\u001b[A\n",
      " 90%|███████████████████████████████████████████▏    | 9/10 [00:02<00:00,  3.49it/s]\u001b[A\n",
      "100%|███████████████████████████████████████████████| 10/10 [00:02<00:00,  3.52it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████████████| 3/3 [00:09<00:00,  3.04s/it]\n",
      "  0%|                                                         | 0/3 [00:00<?, ?it/s]\n",
      "  0%|                                                        | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|████▊                                           | 1/10 [00:00<00:02,  3.39it/s]\u001b[A\n",
      " 20%|█████████▌                                      | 2/10 [00:00<00:02,  3.15it/s]\u001b[A\n",
      " 30%|██████████████▍                                 | 3/10 [00:00<00:02,  3.14it/s]\u001b[A\n",
      " 40%|███████████████████▏                            | 4/10 [00:01<00:01,  3.16it/s]\u001b[A\n",
      " 50%|████████████████████████                        | 5/10 [00:01<00:01,  3.17it/s]\u001b[A\n",
      " 60%|████████████████████████████▊                   | 6/10 [00:01<00:01,  3.18it/s]\u001b[A\n",
      " 70%|█████████████████████████████████▌              | 7/10 [00:02<00:00,  3.17it/s]\u001b[A\n",
      " 80%|██████████████████████████████████████▍         | 8/10 [00:02<00:00,  3.19it/s]\u001b[A\n",
      " 90%|███████████████████████████████████████████▏    | 9/10 [00:02<00:00,  3.20it/s]\u001b[A\n",
      "100%|███████████████████████████████████████████████| 10/10 [00:03<00:00,  3.19it/s]\u001b[A\n",
      " 33%|████████████████▎                                | 1/3 [00:03<00:06,  3.17s/it]\n",
      "  0%|                                                        | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|████▊                                           | 1/10 [00:00<00:02,  3.67it/s]\u001b[A\n",
      " 20%|█████████▌                                      | 2/10 [00:00<00:02,  3.48it/s]\u001b[A\n",
      " 30%|██████████████▍                                 | 3/10 [00:00<00:02,  3.39it/s]\u001b[A\n",
      " 40%|███████████████████▏                            | 4/10 [00:01<00:01,  3.36it/s]\u001b[A\n",
      " 50%|████████████████████████                        | 5/10 [00:01<00:01,  3.34it/s]\u001b[A\n",
      " 60%|████████████████████████████▊                   | 6/10 [00:01<00:01,  3.33it/s]\u001b[A\n",
      " 70%|█████████████████████████████████▌              | 7/10 [00:02<00:00,  3.34it/s]\u001b[A\n",
      " 80%|██████████████████████████████████████▍         | 8/10 [00:02<00:00,  3.33it/s]\u001b[A\n",
      " 90%|███████████████████████████████████████████▏    | 9/10 [00:02<00:00,  3.32it/s]\u001b[A\n",
      "100%|███████████████████████████████████████████████| 10/10 [00:02<00:00,  3.35it/s]\u001b[A\n",
      " 67%|████████████████████████████████▋                | 2/3 [00:06<00:03,  3.08s/it]\n",
      "  0%|                                                        | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|████▊                                           | 1/10 [00:00<00:02,  3.81it/s]\u001b[A\n",
      " 20%|█████████▌                                      | 2/10 [00:00<00:02,  3.65it/s]\u001b[A\n",
      " 30%|██████████████▍                                 | 3/10 [00:00<00:01,  3.52it/s]\u001b[A\n",
      " 40%|███████████████████▏                            | 4/10 [00:01<00:01,  3.51it/s]\u001b[A\n",
      " 50%|████████████████████████                        | 5/10 [00:01<00:01,  3.52it/s]\u001b[A\n",
      " 60%|████████████████████████████▊                   | 6/10 [00:01<00:01,  3.55it/s]\u001b[A\n",
      " 70%|█████████████████████████████████▌              | 7/10 [00:01<00:00,  3.55it/s]\u001b[A\n",
      " 80%|██████████████████████████████████████▍         | 8/10 [00:02<00:00,  3.53it/s]\u001b[A\n",
      " 90%|███████████████████████████████████████████▏    | 9/10 [00:02<00:00,  3.53it/s]\u001b[A\n",
      "100%|███████████████████████████████████████████████| 10/10 [00:02<00:00,  3.55it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████████████| 3/3 [00:09<00:00,  3.01s/it]\n",
      "  0%|                                                         | 0/3 [00:00<?, ?it/s]\n",
      "  0%|                                                        | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|████▊                                           | 1/10 [00:00<00:02,  3.50it/s]\u001b[A\n",
      " 20%|█████████▌                                      | 2/10 [00:00<00:02,  3.32it/s]\u001b[A\n",
      " 30%|██████████████▍                                 | 3/10 [00:00<00:02,  3.26it/s]\u001b[A\n",
      " 40%|███████████████████▏                            | 4/10 [00:01<00:01,  3.26it/s]\u001b[A\n",
      " 50%|████████████████████████                        | 5/10 [00:01<00:01,  3.27it/s]\u001b[A\n",
      " 60%|████████████████████████████▊                   | 6/10 [00:01<00:01,  3.25it/s]\u001b[A\n",
      " 70%|█████████████████████████████████▌              | 7/10 [00:02<00:00,  3.19it/s]\u001b[A\n",
      " 80%|██████████████████████████████████████▍         | 8/10 [00:02<00:00,  3.19it/s]\u001b[A\n",
      " 90%|███████████████████████████████████████████▏    | 9/10 [00:02<00:00,  3.19it/s]\u001b[A\n",
      "100%|███████████████████████████████████████████████| 10/10 [00:03<00:00,  3.24it/s]\u001b[A\n",
      " 33%|████████████████▎                                | 1/3 [00:03<00:06,  3.12s/it]\n",
      "  0%|                                                        | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|████▊                                           | 1/10 [00:00<00:02,  3.61it/s]\u001b[A\n",
      " 20%|█████████▌                                      | 2/10 [00:00<00:02,  3.42it/s]\u001b[A\n",
      " 30%|██████████████▍                                 | 3/10 [00:00<00:02,  3.38it/s]\u001b[A\n",
      " 40%|███████████████████▏                            | 4/10 [00:01<00:01,  3.36it/s]\u001b[A\n",
      " 50%|████████████████████████                        | 5/10 [00:01<00:01,  3.34it/s]\u001b[A\n",
      " 60%|████████████████████████████▊                   | 6/10 [00:01<00:01,  3.33it/s]\u001b[A\n",
      " 70%|█████████████████████████████████▌              | 7/10 [00:02<00:00,  3.34it/s]\u001b[A\n",
      " 80%|██████████████████████████████████████▍         | 8/10 [00:02<00:00,  3.35it/s]\u001b[A\n",
      " 90%|███████████████████████████████████████████▏    | 9/10 [00:02<00:00,  3.33it/s]\u001b[A\n",
      "100%|███████████████████████████████████████████████| 10/10 [00:02<00:00,  3.36it/s]\u001b[A\n",
      " 67%|████████████████████████████████▋                | 2/3 [00:06<00:03,  3.06s/it]\n",
      "  0%|                                                        | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|████▊                                           | 1/10 [00:00<00:02,  3.78it/s]\u001b[A\n",
      " 20%|█████████▌                                      | 2/10 [00:00<00:02,  3.64it/s]\u001b[A\n",
      " 30%|██████████████▍                                 | 3/10 [00:00<00:01,  3.50it/s]\u001b[A\n",
      " 40%|███████████████████▏                            | 4/10 [00:01<00:01,  3.51it/s]\u001b[A\n",
      " 50%|████████████████████████                        | 5/10 [00:01<00:01,  3.50it/s]\u001b[A\n",
      " 60%|████████████████████████████▊                   | 6/10 [00:01<00:01,  3.52it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|█████████████████████████████████▌              | 7/10 [00:01<00:00,  3.50it/s]\u001b[A\n",
      " 80%|██████████████████████████████████████▍         | 8/10 [00:02<00:00,  3.49it/s]\u001b[A\n",
      " 90%|███████████████████████████████████████████▏    | 9/10 [00:02<00:00,  3.52it/s]\u001b[A\n",
      "100%|███████████████████████████████████████████████| 10/10 [00:02<00:00,  3.52it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████████████| 3/3 [00:09<00:00,  3.00s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "\n",
    "index_miss_list = [10, 30, 50]\n",
    "num_of_test = 10\n",
    "miss_methods = [\"MCAR\",\"MAR\",'MNAR']\n",
    "for miss_method in miss_methods:\n",
    "    for index_miss in tqdm(index_miss_list):  \n",
    "        Rimputed_test(miss_method = miss_method, index_miss = index_miss, imputed_method = 'DSAN', num_of_test = num_of_test)\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
