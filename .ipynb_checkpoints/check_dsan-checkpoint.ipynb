{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fa4bdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import json\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33409512",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mapper():\n",
    "    def __init__(self, X, num_vars, cat_vars):\n",
    "        self.num_vars = num_vars\n",
    "        self.cat_vars = cat_vars\n",
    "        self.get_dict(X, num_vars, cat_vars)\n",
    "        \n",
    "    def convert_numeric_feature(self, val):\n",
    "        if np.isnan(val):\n",
    "            return 'NULL'\n",
    "        if val > 2:\n",
    "            return str(int(math.log(val, 2) ** 2))\n",
    "        else:\n",
    "            return str(1)\n",
    "\n",
    "    def convert_categorical_feature(self, val):\n",
    "        if np.isnan(val):\n",
    "            return 'NULL'\n",
    "        return str(int(val))\n",
    "\n",
    "    def get_dict(self, X, num_vars, cat_vars):\n",
    "        f2i = {'OOV': 0}\n",
    "        for idx in num_vars + cat_vars:\n",
    "            key = 'COL:{}:NULL'.format(idx)\n",
    "            f2i[key] = len(f2i)\n",
    "\n",
    "        for idx in num_vars:\n",
    "            for v in X[:, idx].astype('float'):\n",
    "                key = 'COL:{}:{}'.format(idx, self.convert_numeric_feature(v))\n",
    "                if key not in f2i:\n",
    "                    f2i[key] = len(f2i)\n",
    "\n",
    "        for idx in cat_vars:\n",
    "            for v in X[:, idx].astype('float'):\n",
    "                key = 'COL:{}:{}'.format(idx, self.convert_categorical_feature(v))\n",
    "                if key not in f2i:\n",
    "                    f2i[key] = len(f2i)\n",
    "        \n",
    "        self.f2i = f2i\n",
    "    \n",
    "    def convert(self, x):\n",
    "        res = np.zeros(len(self.num_vars)+len(self.cat_vars))\n",
    "        for idx in self.num_vars:\n",
    "            key = 'COL:{}:{}'.format(idx, self.convert_numeric_feature(x[idx]))\n",
    "            res[idx] = self.f2i[key] if key in self.f2i else 0.\n",
    "        for idx in self.cat_vars:\n",
    "            key = 'COL:{}:{}'.format(idx, self.convert_categorical_feature(x[idx]))\n",
    "            res[idx] = self.f2i[key] if key in self.f2i else 0.\n",
    "        return res.astype('int')\n",
    "    \n",
    "    \n",
    "class MVIDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, X, cat_vars, num_vars, scaler, mapper, noise_percent=0):\n",
    "        self.X = X.astype('float')\n",
    "        self.n_col = X.shape[1]\n",
    "        self.cat_vars = cat_vars\n",
    "        self.num_vars = num_vars\n",
    "        self.n2i = {n:i for i, n in enumerate(num_vars)}\n",
    "        self.scaler = scaler\n",
    "        self.mapper = mapper\n",
    "        if noise_percent:\n",
    "            self.n_noise = int(X.shape[1] * noise_percent / 100)\n",
    "        else:\n",
    "            self.n_noise = 0\n",
    "       \n",
    "    def process_num_vars(self, x):\n",
    "        x_num = x[self.num_vars]\n",
    "        x_num = self.scaler.transform(x_num.reshape(1, -1)).squeeze()\n",
    "        mask_num = np.isnan(x_num)\n",
    "        x_num[mask_num] = 0. \n",
    "        num_tensors = torch.from_numpy(x_num).float()\n",
    "        mask_num_tensors = torch.tensor(~mask_num, dtype=torch.int)\n",
    "        return num_tensors, mask_num_tensors\n",
    "\n",
    "    def process_cat_vars(self, x):\n",
    "        cat_tensors = [torch.tensor(v, dtype=torch.long) if not np.isnan(v) else torch.tensor(0)\n",
    "                for v in x[self.cat_vars]]\n",
    "        mask_cat_tensors = [torch.tensor(~pd.isnull(v), dtype=torch.int) for v in x[self.cat_vars]]\n",
    "        return cat_tensors, mask_cat_tensors\n",
    "\n",
    "    def get_noise(self, input_num, token_tensors):\n",
    "        col_idxs = list(range(self.n_col))\n",
    "        if self.n_noise > 0:\n",
    "            noise_idxs = sorted(np.random.choice(col_idxs, size=self.n_noise, replace=False))\n",
    "\n",
    "            for ni in noise_idxs:\n",
    "                if ni in set(self.num_vars):\n",
    "                    input_num[self.n2i[ni]] = 0.\n",
    "                token_tensors[ni] = self.mapper.f2i['COL:{}:NULL'.format(ni)]\n",
    "        return input_num, token_tensors\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.X[index]\n",
    "        tensors = []\n",
    "        masks = []\n",
    "        \n",
    "        num_tensors, mask_num_tensors = self.process_num_vars(x)\n",
    "        input_num = num_tensors.clone().detach()\n",
    "        tensors.append(num_tensors)\n",
    "        masks.append(mask_num_tensors)\n",
    "        \n",
    "        cat_tensors, mask_cat_tensors = self.process_cat_vars(x) \n",
    "        tensors += cat_tensors\n",
    "        masks += mask_cat_tensors\n",
    "        \n",
    "        token_tensors = self.mapper.convert(x)\n",
    "        token_tensors = torch.tensor(token_tensors, dtype=torch.long)\n",
    "        \n",
    "        input_num, token_tensors = self.get_noise(input_num, token_tensors)\n",
    "        \n",
    "        return input_num, token_tensors, tensors, masks\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "\n",
    "class FCUnit(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, dropout=0.0, bias=True):\n",
    "        super(FCUnit, self).__init__()\n",
    "        self.skip_connection = True if input_dim == output_dim else False\n",
    "        self. fc = nn.Sequential(\n",
    "                nn.Linear(input_dim, output_dim, bias=bias),\n",
    "                nn.LayerNorm(output_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout))\n",
    "        \n",
    "        self.apply(self._init_weight)\n",
    "    \n",
    "    def _init_weight(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = self.fc(x)\n",
    "        if self.skip_connection:\n",
    "            return res + x\n",
    "        else:\n",
    "            return res\n",
    "\n",
    "\n",
    "class SelfAttentionUnit(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, max_len, dropout=0.8, bias=False, skip_connection=True):\n",
    "        super(SelfAttentionUnit, self).__init__()\n",
    "        self.skip_connection = skip_connection\n",
    "        self.attn = nn.MultiheadAttention(embed_dim=embed_dim,\n",
    "                num_heads=num_heads,\n",
    "                dropout=dropout,\n",
    "                bias=bias)\n",
    "        self.act = nn.ReLU()\n",
    "        self.ln = nn.LayerNorm([max_len, embed_dim])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(1, 0, 2)\n",
    "        res, _ = self.attn(key=x, value=x, query=x)\n",
    "        res = self.act(res)\n",
    "        if self.skip_connection:\n",
    "            res = res + x\n",
    "        res = res.permute(1, 0, 2)\n",
    "        return self.ln(res)\n",
    "\n",
    "\n",
    "class Featurize(nn.Module):\n",
    "    def __init__(self, n_n, n_c, embed_dim, vocab_size, num_heads, dropout, bias=False):\n",
    "        '''\n",
    "        input: List of Tensors (num vars and cat vars))\n",
    "               num_vars: (bsz, len(num_vars))\n",
    "               token: (bsz, n_col)\n",
    "        output: featurize (bsz, 1+n_c, rep_dim)\n",
    "        '''\n",
    "        super(Featurize, self).__init__()\n",
    "        n_col = n_n + n_c\n",
    "        hidden_dim = (1+n_col) * embed_dim\n",
    "        \n",
    "        self.numeric_fc = FCUnit(input_dim=n_n,\n",
    "                output_dim=embed_dim,\n",
    "                dropout=dropout,\n",
    "                bias=bias)\n",
    "\n",
    "        self.embed = nn.Embedding(num_embeddings=vocab_size,\n",
    "                embedding_dim=embed_dim,\n",
    "                padding_idx=0)\n",
    "\n",
    "        self.attn = SelfAttentionUnit(embed_dim=embed_dim,\n",
    "                num_heads=num_heads,\n",
    "                max_len=n_col,\n",
    "                dropout=dropout,\n",
    "                bias=bias)\n",
    "        \n",
    "        self.apply(self._init_weight)\n",
    "    \n",
    "    def _init_weight(self, m):\n",
    "        if isinstance(m, nn.Embedding):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "    \n",
    "    def forward(self, x_num, tokens):\n",
    "        f_num = self.numeric_fc(x_num)\n",
    "        f_emb = self.embed(tokens)\n",
    "        f_int = self.attn(f_emb).flatten(start_dim=1)\n",
    "        f_tot = torch.cat([f_num, f_int], dim=1)\n",
    "        return f_tot\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, num_vars, cat_vars, tasks, rep_dim, vocab_size, num_heads, n_hidden, dropout):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        n_n = len(num_vars)\n",
    "        n_c = len(cat_vars)\n",
    "        n_col = n_n + n_c\n",
    "        hidden_dim = (1+n_col)*rep_dim\n",
    "\n",
    "        self.featurizer = Featurize(n_n=n_n,\n",
    "                n_c=n_c,\n",
    "                embed_dim=rep_dim,\n",
    "                vocab_size=vocab_size,\n",
    "                num_heads=num_heads,\n",
    "                dropout=dropout)\n",
    "        \n",
    "        shared_unit = [FCUnit(input_dim=hidden_dim,\n",
    "            output_dim=hidden_dim,\n",
    "            dropout=dropout)]\n",
    "\n",
    "        self.shared_unit = nn.Sequential(*shared_unit*n_hidden)\n",
    "\n",
    "        self.multi_task_mappings = nn.ModuleList()\n",
    "        self.multi_task_mappings.append(\n",
    "                nn.Linear(hidden_dim, n_n)\n",
    "        )\n",
    "\n",
    "        for t in tasks:\n",
    "            self.multi_task_mappings.append(\n",
    "                    nn.Linear(hidden_dim, t if t != 2 else 1)\n",
    "            )\n",
    "        \n",
    "        self.apply(self._init_weight)\n",
    "    \n",
    "    def _init_weight(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "    \n",
    "    def forward(self, x_num, tokens):\n",
    "        f_tot = self.featurizer(x_num, tokens)\n",
    "        z = self.shared_unit(f_tot)\n",
    "        outputs = []\n",
    "        for i, task in enumerate(self.multi_task_mappings):\n",
    "            output = task(z)\n",
    "            if i > 0 :\n",
    "                if output.shape[1] == 1:\n",
    "                    output = torch.sigmoid(output)\n",
    "                elif output.shape[1] > 1:\n",
    "                    output = torch.softmax(output, dim=-1)\n",
    "            outputs.append(output)\n",
    "        return outputs\n",
    "\n",
    "                \n",
    "class Imputer(object):\n",
    "\n",
    "    def __init__(self, rep_dim=32, num_heads=8, n_hidden=2,\n",
    "            lr=3e-3, weight_decay=1e-5, batch_size=128,\n",
    "            epochs=10, noise_percent=30, stopped_epoch=10):\n",
    "        \n",
    "        self.rep_dim = rep_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.stopped_epoch = stopped_epoch\n",
    "        self.dropout = 0.5\n",
    "        self.noise_percent = noise_percent\n",
    "\n",
    "    def cal_loss(self, preds, targets, masks):\n",
    "        losses = []\n",
    "        nums = []\n",
    "        eps = 1e-10\n",
    "        \n",
    "        loss_mse = F.mse_loss(preds[0], targets[0], reduction='none')\n",
    "        loss_mse *= masks[0]\n",
    "        N = masks[0].sum()\n",
    "        loss_mse = loss_mse.sum()/(N+eps)\n",
    "        losses.append(loss_mse)\n",
    "        nums.append(N)\n",
    "\n",
    "        for idx, t in enumerate(self.tasks, 1):\n",
    "            if t == 2: # bce\n",
    "                loss = F.binary_cross_entropy(input=preds[idx],\n",
    "                                              target=targets[idx].unsqueeze(-1).float(),\n",
    "                                              reduction='none').reshape(masks[idx].shape)\n",
    "            else: # cce\n",
    "                loss = F.cross_entropy(input=preds[idx],\n",
    "                                       target=targets[idx],\n",
    "                                       reduction='none').reshape(masks[idx].shape)\n",
    "            loss *= masks[idx]\n",
    "            N = masks[idx].sum()\n",
    "            loss = loss.sum()/(N+eps)\n",
    "            losses.append(loss)\n",
    "            nums.append(N)\n",
    "        \n",
    "        losses = torch.stack(losses, 0)\n",
    "        nums = torch.stack(nums, 0)\n",
    "        return losses, nums\n",
    "\n",
    "    def cal_metric_batch(self, preds, targets, masks):\n",
    "        with torch.no_grad():\n",
    "            metric = []\n",
    "\n",
    "            mse = (torch.pow(targets[0] - preds[0], 2) * masks[0]).sum() / masks[0].sum()\n",
    "            metric.append(mse.item())\n",
    "\n",
    "            for idx in range(1, len(preds)):\n",
    "                if preds[idx].shape[1] == 1:\n",
    "                    res = (preds[idx] > 0.5).to(torch.int).squeeze() == targets[idx]\n",
    "                else:\n",
    "                    res = torch.argmax(preds[idx], axis=1) == targets[idx]\n",
    "                acc = (res * masks[idx]).sum() / masks[idx].sum()\n",
    "                metric.append(acc.item())\n",
    "        return metric\n",
    "    \n",
    "    def run_batch(self, batch, epoch, train):\n",
    "        nums, tokens, targets, masks = batch\n",
    "        tokens = tokens.to(DEVICE)\n",
    "        nums = nums.to(DEVICE)\n",
    "        targets = [t.to(DEVICE) for t in targets]\n",
    "        masks = [m.to(DEVICE) for m in masks]\n",
    "        batch_size = targets[0].shape[0]\n",
    "\n",
    "        preds = self.model(nums, tokens)\n",
    "        losses, nums = self.cal_loss(preds, targets, masks)\n",
    "        metric = self.cal_metric_batch(preds, targets, masks)\n",
    "        \n",
    "        loss_tot = losses.mean()\n",
    "        \n",
    "        if train:\n",
    "            self.optimizer.zero_grad()\n",
    "            loss_tot.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "        metric = np.array(metric)\n",
    "        losses = np.array([l.item() for l in losses])\n",
    "        nums = np.array([n.item() for n in nums])\n",
    "        \n",
    "        return loss_tot.item(), metric, losses, nums, batch_size\n",
    "\n",
    "    def train(self, epoch=None):\n",
    "        self.model.train()\n",
    "        t_loss_tot, cnt = 0., 0.\n",
    "        n_task = len(self.tasks)+1\n",
    "        t_metrics = np.zeros(n_task)\n",
    "        t_losses = np.zeros(n_task)\n",
    "        mask_cnt = np.zeros(n_task)\n",
    "\n",
    "        for batch_idx, batch in enumerate(self.train_iter):\n",
    "            loss_tot, metric, losses, nums, batch_size = self.run_batch(batch, epoch, train=True)\n",
    "            t_metrics += metric * nums\n",
    "            t_loss_tot += loss_tot * batch_size\n",
    "            cnt += batch_size\n",
    "            t_losses += losses * nums\n",
    "            mask_cnt += nums\n",
    "            \n",
    "        status = {\n",
    "            'task_metrics' : t_metrics/mask_cnt,\n",
    "            'task_losses' : t_losses/mask_cnt,\n",
    "            'total_loss' : t_loss_tot/cnt\n",
    "        }\n",
    "        \n",
    "        return status\n",
    "    \n",
    "    def fit(self, X, cat_vars=None):\n",
    "        \n",
    "        self.n_col = X.shape[1]\n",
    "        self.cat_vars = cat_vars if cat_vars else list()\n",
    "        num_vars = set(range(self.n_col)) - set(self.cat_vars)\n",
    "        num_vars = list(num_vars)\n",
    "        self.num_vars = num_vars\n",
    "        \n",
    "        self.c2is = None\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(X[:, self.num_vars])\n",
    "        mapper = Mapper(X=X, num_vars=num_vars, cat_vars=cat_vars)\n",
    "        tasks = [int(np.nan_to_num(X[:, idx].astype('float')).max()+1) for idx in cat_vars] \n",
    "        \n",
    "        self.scaler = scaler\n",
    "        self.mapper = mapper\n",
    "        self.tasks = tasks\n",
    "\n",
    "        self.model = Model(num_vars=num_vars,\n",
    "                cat_vars=cat_vars,\n",
    "                tasks=tasks,\n",
    "                rep_dim=self.rep_dim,\n",
    "                vocab_size=len(mapper.f2i),\n",
    "                num_heads=self.num_heads,\n",
    "                n_hidden=self.n_hidden,\n",
    "                dropout=self.dropout).to(DEVICE)\n",
    "       \n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(),\n",
    "                                          lr=self.lr,\n",
    "                                          weight_decay=self.weight_decay)\n",
    "\n",
    "        dataset = MVIDataset(X, cat_vars, num_vars, scaler, mapper, self.noise_percent)\n",
    "        self.train_iter = DataLoader(dataset=dataset,\n",
    "                                     batch_size=self.batch_size,\n",
    "                                     shuffle=True,\n",
    "                                     num_workers=0)\n",
    "        \n",
    "        wait = 0\n",
    "        best_loss = None\n",
    "        iteration = range(1, self.epochs+1)\n",
    "        \n",
    "        for epoch in iteration:\n",
    "            epoch_status = self.train(epoch)\n",
    "            \n",
    "            if self.stopped_epoch:\n",
    "                if best_loss is None or best_loss > epoch_status['total_loss']:\n",
    "                    best_loss = epoch_status['total_loss']\n",
    "                    wait = 0\n",
    "                else:\n",
    "                    wait += 1\n",
    "                \n",
    "                if wait > self.stopped_epoch:\n",
    "                    print('Terminated Training for Early Stopping at Epoch {}'.format(epoch))\n",
    "                    break\n",
    "            \n",
    "        return self\n",
    "\n",
    "    def np_inverse_transform(self, array, dic):\n",
    "        u, inv = np.unique(array, return_inverse=True)\n",
    "        return np.array([dic[x] for x in u])[inv].reshape(array.shape)\n",
    "    \n",
    "    def _transform(self, outputs):\n",
    "        batch_size = outputs[0].shape[0]\n",
    "        rec_num_vars = self.scaler.inverse_transform(outputs[0].cpu().detach().numpy())\n",
    "        rec_cat_vars = []\n",
    "        for output in outputs[1:]:\n",
    "            if output.shape[1] == 1:\n",
    "                output = (output.cpu().detach().numpy() > 0.5).astype('int')\n",
    "            else:\n",
    "                output = np.argmax(output.cpu().detach().numpy(), axis=1).reshape(-1, 1)\n",
    "            rec_cat_vars.append(output.astype('object'))\n",
    "        if len(rec_cat_vars) > 0:\n",
    "            rec_cat_vars = np.concatenate(rec_cat_vars, axis=1)\n",
    "\n",
    "        result = np.zeros((batch_size, self.n_col))\n",
    "        result[:, self.num_vars] = rec_num_vars\n",
    "        result = result.astype('object')\n",
    "        result[:, self.cat_vars] = rec_cat_vars\n",
    "        return result\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \n",
    "        dataset = MVIDataset(X, self.cat_vars, self.num_vars, self.scaler, self.mapper)\n",
    "        impute_iter = DataLoader(dataset=dataset,\n",
    "                                 batch_size = self.batch_size,\n",
    "                                 shuffle=False,\n",
    "                                 num_workers=0)\n",
    "        \n",
    "        X_imputed = []\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, batch in enumerate(impute_iter):\n",
    "                nums, tokens, _, _ = batch\n",
    "                nums = nums.to(DEVICE)\n",
    "                tokens = tokens.to(DEVICE)\n",
    "                \n",
    "                outputs = self.model(nums, tokens)\n",
    "                result = self._transform(outputs)\n",
    "                X_imputed.append(result)\n",
    "        \n",
    "        return np.vstack(X_imputed)\n",
    "\n",
    "    def fit_transform(self, X, **fit_params):\n",
    "\n",
    "        return self.fit(X, **fit_params).transform(X)\n",
    "\n",
    "\n",
    "class MVImputer(object):\n",
    "\n",
    "    def __init__(self, input_fname, result_path, result_csv_path):\n",
    "        self.label_encoders = defaultdict(LabelEncoder)\n",
    "        self.get_data(input_fname)\n",
    "        self.input_fname = input_fname\n",
    "        self.result_path = result_path\n",
    "        self.result_csv_path = result_csv_path\n",
    "\n",
    "    def get_data(self, input_fname):\n",
    "        data = pd.read_csv(input_fname)\n",
    "        na_loc = data.isnull()\n",
    "        for idx in data.columns:\n",
    "            if idx.startswith('cat'):\n",
    "                data[idx] = data[idx].astype(str)\n",
    "        num_idx = data.dtypes[data.dtypes != 'object'].index\n",
    "        num_vars = [data.columns.get_loc(idx) for idx in num_idx]\n",
    "        cat_vars = list(set(range(data.shape[1])) - set(num_vars))\n",
    "        data[na_loc] = np.nan\n",
    "        \n",
    "        self.data = data\n",
    "        self.num_vars = num_vars\n",
    "        self.cat_vars = cat_vars\n",
    "    \n",
    "    def label_encode(self, data):\n",
    "        missing = pd.isnull(data.iloc[:, self.cat_vars])\n",
    "        encode_values = data.iloc[:, self.cat_vars].fillna('NULL')\n",
    "        encode_values = encode_values.apply(\n",
    "                lambda x: self.label_encoders[x.name].fit_transform(x)\n",
    "                )\n",
    "        encode_values = encode_values.values.astype('float')\n",
    "        encode_values[missing] = np.nan\n",
    "        data.iloc[:, self.cat_vars] = encode_values\n",
    "        return data\n",
    "\n",
    "    def inverse_transform(self, data):\n",
    "        inverse_values = data.iloc[:, self.cat_vars].astype('int')\n",
    "        inverse_values = inverse_values.apply(\n",
    "                lambda x: self.label_encoders[x.name].inverse_transform(x)\n",
    "                ) \n",
    "        data.iloc[:, self.cat_vars] = inverse_values.values\n",
    "        return data\n",
    "\n",
    "    def run(self):\n",
    "\n",
    "        if not os.path.isdir(self.result_path):\n",
    "            os.makedirs(self.result_path)\n",
    "\n",
    "        data = self.label_encode(self.data)\n",
    "        X_incomplete = data.values\n",
    "        targets = pd.isnull(X_incomplete)\n",
    "        \n",
    "        imputer = Imputer()\n",
    "        X_imputed = imputer.fit_transform(X_incomplete, cat_vars=self.cat_vars)\n",
    "        imputed_values = X_incomplete.copy()\n",
    "        imputed_values[targets] = X_imputed[targets]\n",
    "        result_data = pd.DataFrame(imputed_values, columns=data.columns)\n",
    "        result_data = self.inverse_transform(result_data)\n",
    "\n",
    "        result_data.to_csv(self.result_csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1004edc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_encode(data):\n",
    "    missing = pd.isnull(data.iloc[:,cat_vars])\n",
    "    encode_values = data.iloc[:,cat_vars].fillna('NULL')\n",
    "    encode_values = encode_values.apply(\n",
    "            lambda x: label_encoders[x.name].fit_transform(x)\n",
    "            )\n",
    "    encode_values = encode_values.values.astype('float')\n",
    "    encode_values[missing] = np.nan\n",
    "    data.iloc[:, self.cat_vars] = encode_values\n",
    "    return data\n",
    "\n",
    "class Mapper():\n",
    "    def __init__(self, X, num_vars, cat_vars):\n",
    "        self.num_vars = num_vars\n",
    "        self.cat_vars = cat_vars\n",
    "        self.get_dict(X, num_vars, cat_vars)\n",
    "        \n",
    "    def convert_numeric_feature(self, val):\n",
    "        if np.isnan(val):\n",
    "            return 'NULL'\n",
    "        if val > 2:\n",
    "            return str(int(math.log(val, 2) ** 2))\n",
    "        else:\n",
    "            return str(1)\n",
    "\n",
    "    def convert_categorical_feature(self, val):\n",
    "        if np.isnan(val):\n",
    "            return 'NULL'\n",
    "        return str(int(val))\n",
    "\n",
    "    def get_dict(self, X, num_vars, cat_vars):\n",
    "        f2i = {'OOV': 0}\n",
    "        for idx in num_vars + cat_vars:\n",
    "            key = 'COL:{}:NULL'.format(idx)\n",
    "            f2i[key] = len(f2i)\n",
    "\n",
    "        for idx in num_vars:\n",
    "            for v in X[:, idx].astype('float'):\n",
    "                key = 'COL:{}:{}'.format(idx, self.convert_numeric_feature(v))\n",
    "                if key not in f2i:\n",
    "                    f2i[key] = len(f2i)\n",
    "\n",
    "        for idx in cat_vars:\n",
    "            for v in X[:, idx].astype('float'):\n",
    "                key = 'COL:{}:{}'.format(idx, self.convert_categorical_feature(v))\n",
    "                if key not in f2i:\n",
    "                    f2i[key] = len(f2i)\n",
    "        \n",
    "        self.f2i = f2i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc7dd72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoders = defaultdict(LabelEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43015c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./data_stored/data_miss/MCAR/Case14/miss10/0.csv')\n",
    "na_loc = data.isnull()\n",
    "for idx in data.columns:\n",
    "    if idx.startswith('cat'):\n",
    "        data[idx] = data[idx].astype(str)\n",
    "num_idx = data.dtypes[data.dtypes != 'object'].index\n",
    "num_vars = [data.columns.get_loc(idx) for idx in num_idx]\n",
    "cat_vars = list(set(range(data.shape[1])) - set(num_vars))\n",
    "data[na_loc] = np.nan\n",
    "\n",
    "data = data\n",
    "num_vars = num_vars\n",
    "cat_vars = cat_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a3ddeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing = pd.isnull(data.iloc[:,cat_vars])\n",
    "encode_values = data.iloc[:,cat_vars].fillna('NULL')\n",
    "encode_values = encode_values.apply(\n",
    "        lambda x: label_encoders[x.name].fit_transform(x)\n",
    "        )\n",
    "encode_values = encode_values.values.astype('float')\n",
    "encode_values[missing] = np.nan\n",
    "data.iloc[:, cat_vars] = encode_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1e9b9873",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_incomplete = data.values\n",
    "targets = pd.isnull(X_incomplete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3383bfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_dim=32\n",
    "num_heads=8\n",
    "n_hidden=2\n",
    "lr=3e-3\n",
    "weight_decay=1e-5\n",
    "batch_size=128\n",
    "epochs=10\n",
    "noise_percent=30\n",
    "stopped_epoch=10\n",
    "dropout = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "57d31c1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 8, 9, 10, 11]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "36c683b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# c2is = None\n",
    "# scaler = StandardScaler()\n",
    "# scaler.fit(X_incomplete[:, num_vars])\n",
    "# mapper = Mapper(X=X_incomplete, num_vars=num_vars, cat_vars=cat_vars)\n",
    "tasks = [int(np.nan_to_num(X_incomplete[:, idx].astype('float')).max()+1) for idx in cat_vars] \n",
    "\n",
    "# self.scaler = scaler\n",
    "# self.mapper = mapper\n",
    "# self.tasks = tasks\n",
    "\n",
    "# self.model = Model(num_vars=num_vars,\n",
    "#         cat_vars=cat_vars,\n",
    "#         tasks=tasks,\n",
    "#         rep_dim=self.rep_dim,\n",
    "#         vocab_size=len(mapper.f2i),\n",
    "#         num_heads=self.num_heads,\n",
    "#         n_hidden=self.n_hidden,\n",
    "#         dropout=self.dropout).to(DEVICE)\n",
    "\n",
    "# self.optimizer = torch.optim.Adam(self.model.parameters(),\n",
    "#                                   lr=self.lr,\n",
    "#                                   weight_decay=self.weight_decay)\n",
    "\n",
    "# dataset = MVIDataset(X, cat_vars, num_vars, scaler, mapper, self.noise_percent)\n",
    "# self.train_iter = DataLoader(dataset=dataset,\n",
    "#                              batch_size=self.batch_size,\n",
    "#                              shuffle=True,\n",
    "#                              num_workers=0)\n",
    "\n",
    "# wait = 0\n",
    "# best_loss = None\n",
    "# iteration = range(1, self.epochs+1)\n",
    "\n",
    "# for epoch in iteration:\n",
    "#     epoch_status = self.train(epoch)\n",
    "\n",
    "#     if self.stopped_epoch:\n",
    "#         if best_loss is None or best_loss > epoch_status['total_loss']:\n",
    "#             best_loss = epoch_status['total_loss']\n",
    "#             wait = 0\n",
    "#         else:\n",
    "#             wait += 1\n",
    "\n",
    "#         if wait > self.stopped_epoch:\n",
    "#             print('Terminated Training for Early Stopping at Epoch {}'.format(epoch))\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "69dd8e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                         | 0/3 [00:00<?, ?it/s]\n",
      "  0%|                                                         | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "  0%|                                                        | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|████▊                                           | 1/10 [00:15<02:17, 15.25s/it]\u001b[A\u001b[A\n",
      "\n",
      " 20%|█████████▌                                      | 2/10 [00:30<02:01, 15.15s/it]\u001b[A\u001b[A\n",
      "\n",
      " 30%|██████████████▍                                 | 3/10 [00:46<01:49, 15.60s/it]\u001b[A\u001b[A\n",
      "\n",
      " 40%|███████████████████▏                            | 4/10 [01:01<01:33, 15.54s/it]\u001b[A\u001b[A\n",
      "\n",
      " 50%|████████████████████████                        | 5/10 [01:17<01:17, 15.41s/it]\u001b[A\u001b[A\n",
      "\n",
      " 60%|████████████████████████████▊                   | 6/10 [01:32<01:02, 15.52s/it]\u001b[A\u001b[A\n",
      "\n",
      " 70%|█████████████████████████████████▌              | 7/10 [01:47<00:46, 15.40s/it]\u001b[A\u001b[A\n",
      "\n",
      " 80%|██████████████████████████████████████▍         | 8/10 [02:03<00:30, 15.35s/it]\u001b[A\u001b[A\n",
      "\n",
      " 90%|███████████████████████████████████████████▏    | 9/10 [02:18<00:15, 15.18s/it]\u001b[A\u001b[A\n",
      "\n",
      "100%|███████████████████████████████████████████████| 10/10 [02:33<00:00, 15.30s/it]\u001b[A\u001b[A\n",
      "\n",
      " 33%|████████████████                                | 1/3 [02:33<05:06, 153.01s/it]\u001b[A\n",
      "\n",
      "  0%|                                                        | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|████▊                                           | 1/10 [00:15<02:18, 15.39s/it]\u001b[A\u001b[A\n",
      "\n",
      " 20%|█████████▌                                      | 2/10 [00:30<02:03, 15.46s/it]\u001b[A\u001b[A\n",
      "\n",
      " 30%|██████████████▍                                 | 3/10 [00:46<01:47, 15.38s/it]\u001b[A\u001b[A\n",
      "\n",
      " 40%|███████████████████▏                            | 4/10 [01:01<01:31, 15.20s/it]\u001b[A\u001b[A\n",
      "\n",
      " 50%|████████████████████████                        | 5/10 [01:16<01:16, 15.29s/it]\u001b[A\u001b[A\n",
      "\n",
      " 60%|████████████████████████████▊                   | 6/10 [01:31<01:00, 15.15s/it]\u001b[A\u001b[A\n",
      "\n",
      " 70%|█████████████████████████████████▌              | 7/10 [01:46<00:45, 15.25s/it]\u001b[A\u001b[A\n",
      "\n",
      " 80%|██████████████████████████████████████▍         | 8/10 [02:01<00:30, 15.17s/it]\u001b[A\u001b[A\n",
      "\n",
      " 90%|███████████████████████████████████████████▏    | 9/10 [02:16<00:15, 15.09s/it]\u001b[A\u001b[A\n",
      "\n",
      "100%|███████████████████████████████████████████████| 10/10 [02:31<00:00, 15.19s/it]\u001b[A\u001b[A\n",
      "\n",
      " 67%|████████████████████████████████                | 2/3 [05:04<02:32, 152.38s/it]\u001b[A\n",
      "\n",
      "  0%|                                                        | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|████▊                                           | 1/10 [00:15<02:17, 15.24s/it]\u001b[A\u001b[A\n",
      "\n",
      " 20%|█████████▌                                      | 2/10 [00:30<02:01, 15.25s/it]\u001b[A\u001b[A\n",
      "\n",
      " 30%|█████████████▌                               | 3/10 [32:22<1:42:48, 881.27s/it]\u001b[A\u001b[A\n",
      "\n",
      " 40%|██████████████████▊                            | 4/10 [32:38<53:57, 539.57s/it]\u001b[A\u001b[A\n",
      "\n",
      " 50%|███████████████████████▌                       | 5/10 [32:52<29:11, 350.34s/it]\u001b[A\u001b[A\n",
      "\n",
      " 60%|████████████████████████████▏                  | 6/10 [33:07<15:44, 236.21s/it]\u001b[A\u001b[A\n",
      "\n",
      " 70%|████████████████████████████████▉              | 7/10 [33:22<08:12, 164.00s/it]\u001b[A\u001b[A\n",
      "\n",
      " 80%|█████████████████████████████████████▌         | 8/10 [33:37<03:52, 116.47s/it]\u001b[A\u001b[A\n",
      "\n",
      " 90%|███████████████████████████████████████████▏    | 9/10 [33:52<01:24, 84.68s/it]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████████████████████████████████████████| 10/10 [34:07<00:00, 204.71s/it]\u001b[A\u001b[A\n",
      "\n",
      "100%|████████████████████████████████████████████████| 3/3 [39:12<00:00, 784.02s/it]\u001b[A\n",
      " 33%|███████████████                              | 1/3 [39:12<1:18:24, 2352.05s/it]\n",
      "  0%|                                                         | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "  0%|                                                        | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|████▊                                           | 1/10 [00:15<02:16, 15.21s/it]\u001b[A\u001b[A\n",
      "\n",
      " 20%|█████████▌                                      | 2/10 [00:30<01:59, 15.00s/it]\u001b[A\u001b[A\n",
      "\n",
      " 30%|██████████████▍                                 | 3/10 [00:44<01:44, 14.89s/it]\u001b[A\u001b[A\n",
      "\n",
      " 40%|██████████████████                           | 4/10 [33:07<1:17:34, 775.78s/it]\u001b[A\u001b[A\n",
      "\n",
      " 50%|██████████████████████▌                      | 5/10 [44:26<1:01:44, 740.97s/it]\u001b[A\u001b[A\n",
      "\n",
      " 60%|████████████████████████████▏                  | 6/10 [44:41<32:56, 494.17s/it]\u001b[A\u001b[A\n",
      "\n",
      " 70%|████████████████████████████████▉              | 7/10 [44:56<16:52, 337.41s/it]\u001b[A\u001b[A\n",
      "\n",
      " 80%|█████████████████████████████████████▌         | 8/10 [45:12<07:50, 235.25s/it]\u001b[A\u001b[A\n",
      "\n",
      " 90%|██████████████████████████████████████████▎    | 9/10 [45:28<02:46, 166.58s/it]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████████████████████████████████████████| 10/10 [45:44<00:00, 274.43s/it]\u001b[A\u001b[A\n",
      "\n",
      " 33%|███████████████                              | 1/3 [45:44<1:31:28, 2744.28s/it]\u001b[A\n",
      "\n",
      "  0%|                                                        | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|████▊                                           | 1/10 [00:15<02:23, 15.98s/it]\u001b[A\u001b[A\n",
      "\n",
      " 20%|█████████▌                                      | 2/10 [00:32<02:10, 16.34s/it]\u001b[A\u001b[A\n",
      "\n",
      " 30%|██████████████▍                                 | 3/10 [00:48<01:51, 15.99s/it]\u001b[A\u001b[A\n",
      "\n",
      " 40%|███████████████████▏                            | 4/10 [01:03<01:34, 15.68s/it]\u001b[A\u001b[A\n",
      "\n",
      " 50%|████████████████████████                        | 5/10 [01:18<01:18, 15.62s/it]\u001b[A\u001b[A\n",
      "\n",
      " 60%|████████████████████████████▊                   | 6/10 [01:36<01:05, 16.32s/it]\u001b[A\u001b[A\n",
      "\n",
      " 70%|█████████████████████████████████▌              | 7/10 [01:53<00:49, 16.58s/it]\u001b[A\u001b[A\n",
      "\n",
      " 80%|██████████████████████████████████████▍         | 8/10 [02:09<00:32, 16.25s/it]\u001b[A\u001b[A\n",
      "\n",
      " 90%|███████████████████████████████████████████▏    | 9/10 [02:24<00:15, 15.82s/it]\u001b[A\u001b[A\n",
      "\n",
      "100%|███████████████████████████████████████████████| 10/10 [02:39<00:00, 15.91s/it]\u001b[A\u001b[A\n",
      "\n",
      " 67%|███████████████████████████████▎               | 2/3 [48:23<20:23, 1223.56s/it]\u001b[A\n",
      "\n",
      "  0%|                                                        | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|████▊                                           | 1/10 [00:15<02:15, 15.04s/it]\u001b[A\u001b[A\n",
      "\n",
      " 20%|█████████▌                                      | 2/10 [00:29<01:59, 14.98s/it]\u001b[A\u001b[A\n",
      "\n",
      " 30%|██████████████▍                                 | 3/10 [00:45<01:45, 15.14s/it]\u001b[A\u001b[A\n",
      "\n",
      " 40%|███████████████████▏                            | 4/10 [01:00<01:30, 15.15s/it]\u001b[A\u001b[A\n",
      "\n",
      " 50%|████████████████████████                        | 5/10 [01:15<01:15, 15.12s/it]\u001b[A\u001b[A\n",
      "\n",
      " 60%|████████████████████████████▊                   | 6/10 [01:31<01:01, 15.25s/it]\u001b[A\u001b[A\n",
      "\n",
      " 70%|█████████████████████████████████▌              | 7/10 [04:01<02:58, 59.35s/it]\u001b[A\u001b[A\n",
      "\n",
      " 80%|██████████████████████████████████████▍         | 8/10 [04:16<01:30, 45.21s/it]\u001b[A\u001b[A\n",
      "\n",
      " 90%|███████████████████████████████████████████▏    | 9/10 [04:31<00:35, 35.90s/it]\u001b[A\u001b[A\n",
      "\n",
      "100%|███████████████████████████████████████████████| 10/10 [04:47<00:00, 28.71s/it]\u001b[A\u001b[A\n",
      "\n",
      "100%|███████████████████████████████████████████████| 3/3 [53:10<00:00, 1063.49s/it]\u001b[A\n",
      " 67%|██████████████████████████████               | 2/3 [1:32:22<47:25, 2845.24s/it]\n",
      "  0%|                                                         | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "  0%|                                                        | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|████▊                                           | 1/10 [00:15<02:23, 15.93s/it]\u001b[A\u001b[A\n",
      "\n",
      " 20%|█████████▌                                      | 2/10 [00:31<02:04, 15.60s/it]\u001b[A\u001b[A\n",
      "\n",
      " 30%|██████████████▍                                 | 3/10 [00:46<01:46, 15.27s/it]\u001b[A\u001b[A\n",
      "\n",
      " 40%|███████████████████▏                            | 4/10 [01:01<01:31, 15.20s/it]\u001b[A\u001b[A\n",
      "\n",
      " 50%|████████████████████████                        | 5/10 [01:16<01:16, 15.22s/it]\u001b[A\u001b[A\n",
      "\n",
      " 60%|████████████████████████████▊                   | 6/10 [01:31<01:00, 15.12s/it]\u001b[A\u001b[A\n",
      "\n",
      " 70%|█████████████████████████████████▌              | 7/10 [01:46<00:45, 15.06s/it]\u001b[A\u001b[A\n",
      "\n",
      " 80%|██████████████████████████████████████▍         | 8/10 [02:01<00:29, 14.96s/it]\u001b[A\u001b[A\n",
      "\n",
      " 90%|███████████████████████████████████████████▏    | 9/10 [02:16<00:15, 15.09s/it]\u001b[A\u001b[A\n",
      "\n",
      "100%|███████████████████████████████████████████████| 10/10 [02:31<00:00, 15.16s/it]\u001b[A\u001b[A\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|████████████████                                | 1/3 [02:31<05:03, 151.63s/it]\u001b[A\n",
      "\n",
      "  0%|                                                        | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|████▊                                           | 1/10 [00:16<02:30, 16.67s/it]\u001b[A\u001b[A\n",
      "\n",
      " 20%|█████████▌                                      | 2/10 [00:31<02:05, 15.73s/it]\u001b[A\u001b[A\n",
      "\n",
      " 30%|██████████████▍                                 | 3/10 [00:46<01:48, 15.45s/it]\u001b[A\u001b[A\n",
      "\n",
      " 40%|███████████████████▏                            | 4/10 [01:02<01:32, 15.39s/it]\u001b[A\u001b[A\n",
      "\n",
      " 50%|████████████████████████                        | 5/10 [01:17<01:17, 15.44s/it]\u001b[A\u001b[A\n",
      "\n",
      " 60%|████████████████████████████▊                   | 6/10 [01:32<01:01, 15.35s/it]\u001b[A\u001b[A\n",
      "\n",
      " 70%|█████████████████████████████████▌              | 7/10 [01:48<00:46, 15.35s/it]\u001b[A\u001b[A\n",
      "\n",
      " 80%|██████████████████████████████████████▍         | 8/10 [02:03<00:30, 15.27s/it]\u001b[A\u001b[A\n",
      "\n",
      " 90%|███████████████████████████████████████████▏    | 9/10 [02:18<00:15, 15.15s/it]\u001b[A\u001b[A\n",
      "\n",
      "100%|███████████████████████████████████████████████| 10/10 [02:34<00:00, 15.41s/it]\u001b[A\u001b[A\n",
      "\n",
      " 67%|████████████████████████████████                | 2/3 [05:05<02:33, 153.08s/it]\u001b[A\n",
      "\n",
      "  0%|                                                        | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|████▊                                           | 1/10 [00:16<02:30, 16.71s/it]\u001b[A\u001b[A\n",
      "\n",
      " 20%|█████████▌                                      | 2/10 [00:33<02:11, 16.48s/it]\u001b[A\u001b[A\n",
      "\n",
      " 30%|██████████████▍                                 | 3/10 [00:48<01:53, 16.16s/it]\u001b[A\u001b[A\n",
      "\n",
      " 40%|███████████████████▏                            | 4/10 [01:03<01:34, 15.75s/it]\u001b[A\u001b[A\n",
      "\n",
      " 50%|████████████████████████                        | 5/10 [01:19<01:18, 15.68s/it]\u001b[A\u001b[A\n",
      "\n",
      " 60%|████████████████████████████▊                   | 6/10 [01:35<01:02, 15.65s/it]\u001b[A\u001b[A\n",
      "\n",
      " 70%|█████████████████████████████████▌              | 7/10 [01:50<00:47, 15.73s/it]\u001b[A\u001b[A\n",
      "\n",
      " 80%|██████████████████████████████████████▍         | 8/10 [02:07<00:31, 15.84s/it]\u001b[A\u001b[A\n",
      "\n",
      " 90%|███████████████████████████████████████████▏    | 9/10 [02:22<00:15, 15.67s/it]\u001b[A\u001b[A\n",
      "\n",
      "100%|███████████████████████████████████████████████| 10/10 [02:37<00:00, 15.76s/it]\u001b[A\u001b[A\n",
      "\n",
      "100%|████████████████████████████████████████████████| 3/3 [07:43<00:00, 154.43s/it]\u001b[A\n",
      "100%|█████████████████████████████████████████████| 3/3 [1:40:05<00:00, 2001.94s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import json\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "class Mapper():\n",
    "    def __init__(self, X, num_vars, cat_vars):\n",
    "        self.num_vars = num_vars\n",
    "        self.cat_vars = cat_vars\n",
    "        self.get_dict(X, num_vars, cat_vars)\n",
    "        \n",
    "    def convert_numeric_feature(self, val):\n",
    "        if np.isnan(val):\n",
    "            return 'NULL'\n",
    "        if val > 2:\n",
    "            return str(int(math.log(val, 2) ** 2))\n",
    "        else:\n",
    "            return str(1)\n",
    "\n",
    "    def convert_categorical_feature(self, val):\n",
    "        if np.isnan(val):\n",
    "            return 'NULL'\n",
    "        return str(int(val))\n",
    "\n",
    "    def get_dict(self, X, num_vars, cat_vars):\n",
    "        f2i = {'OOV': 0}\n",
    "        for idx in num_vars + cat_vars:\n",
    "            key = 'COL:{}:NULL'.format(idx)\n",
    "            f2i[key] = len(f2i)\n",
    "\n",
    "        for idx in num_vars:\n",
    "            for v in X[:, idx].astype('float'):\n",
    "                key = 'COL:{}:{}'.format(idx, self.convert_numeric_feature(v))\n",
    "                if key not in f2i:\n",
    "                    f2i[key] = len(f2i)\n",
    "\n",
    "        for idx in cat_vars:\n",
    "            for v in X[:, idx].astype('float'):\n",
    "                key = 'COL:{}:{}'.format(idx, self.convert_categorical_feature(v))\n",
    "                if key not in f2i:\n",
    "                    f2i[key] = len(f2i)\n",
    "        \n",
    "        self.f2i = f2i\n",
    "    \n",
    "    def convert(self, x):\n",
    "        res = np.zeros(len(self.num_vars)+len(self.cat_vars))\n",
    "        for idx in self.num_vars:\n",
    "            key = 'COL:{}:{}'.format(idx, self.convert_numeric_feature(x[idx]))\n",
    "            res[idx] = self.f2i[key] if key in self.f2i else 0.\n",
    "        for idx in self.cat_vars:\n",
    "            key = 'COL:{}:{}'.format(idx, self.convert_categorical_feature(x[idx]))\n",
    "            res[idx] = self.f2i[key] if key in self.f2i else 0.\n",
    "        return res.astype('int')\n",
    "    \n",
    "    \n",
    "class MVIDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, X, cat_vars, num_vars, scaler, mapper, noise_percent=0):\n",
    "        self.X = X.astype('float')\n",
    "        self.n_col = X.shape[1]\n",
    "        self.cat_vars = cat_vars\n",
    "        self.num_vars = num_vars\n",
    "        self.n2i = {n:i for i, n in enumerate(num_vars)}\n",
    "        self.scaler = scaler\n",
    "        self.mapper = mapper\n",
    "        if noise_percent:\n",
    "            self.n_noise = int(X.shape[1] * noise_percent / 100)\n",
    "        else:\n",
    "            self.n_noise = 0\n",
    "       \n",
    "    def process_num_vars(self, x):\n",
    "        x_num = x[self.num_vars]\n",
    "        x_num = self.scaler.transform(x_num.reshape(1, -1)).squeeze()\n",
    "        mask_num = np.isnan(x_num)\n",
    "        x_num[mask_num] = 0. \n",
    "        num_tensors = torch.from_numpy(x_num).float()\n",
    "        mask_num_tensors = torch.tensor(~mask_num, dtype=torch.int)\n",
    "        return num_tensors, mask_num_tensors\n",
    "\n",
    "    def process_cat_vars(self, x):\n",
    "        cat_tensors = [torch.tensor(v, dtype=torch.long) if not np.isnan(v) else torch.tensor(0)\n",
    "                for v in x[self.cat_vars]]\n",
    "        mask_cat_tensors = [torch.tensor(~np.isnan(v), dtype=torch.int) for v in x[self.cat_vars]]\n",
    "        return cat_tensors, mask_cat_tensors\n",
    "\n",
    "    def get_noise(self, input_num, token_tensors):\n",
    "        col_idxs = list(range(self.n_col))\n",
    "        if self.n_noise > 0:\n",
    "            noise_idxs = sorted(np.random.choice(col_idxs, size=self.n_noise, replace=False))\n",
    "\n",
    "            for ni in noise_idxs:\n",
    "                if ni in set(self.num_vars):\n",
    "                    input_num[self.n2i[ni]] = 0.\n",
    "                token_tensors[ni] = self.mapper.f2i['COL:{}:NULL'.format(ni)]\n",
    "        return input_num, token_tensors\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.X[index]\n",
    "        tensors = []\n",
    "        masks = []\n",
    "        \n",
    "        num_tensors, mask_num_tensors = self.process_num_vars(x)\n",
    "        input_num = num_tensors.clone().detach()\n",
    "        tensors.append(num_tensors)\n",
    "        masks.append(mask_num_tensors)\n",
    "        \n",
    "        cat_tensors, mask_cat_tensors = self.process_cat_vars(x) \n",
    "        tensors += cat_tensors\n",
    "        masks += mask_cat_tensors\n",
    "        \n",
    "        token_tensors = self.mapper.convert(x)\n",
    "        token_tensors = torch.tensor(token_tensors, dtype=torch.long)\n",
    "        \n",
    "        input_num, token_tensors = self.get_noise(input_num, token_tensors)\n",
    "        \n",
    "        return input_num, token_tensors, tensors, masks\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "\n",
    "class FCUnit(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, dropout=0.0, bias=True):\n",
    "        super(FCUnit, self).__init__()\n",
    "        self.skip_connection = True if input_dim == output_dim else False\n",
    "        self. fc = nn.Sequential(\n",
    "                nn.Linear(input_dim, output_dim, bias=bias),\n",
    "                nn.LayerNorm(output_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout))\n",
    "        \n",
    "        self.apply(self._init_weight)\n",
    "    \n",
    "    def _init_weight(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = self.fc(x)\n",
    "        if self.skip_connection:\n",
    "            return res + x\n",
    "        else:\n",
    "            return res\n",
    "\n",
    "\n",
    "class SelfAttentionUnit(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, max_len, dropout=0.8, bias=False, skip_connection=True):\n",
    "        super(SelfAttentionUnit, self).__init__()\n",
    "        self.skip_connection = skip_connection\n",
    "        self.attn = nn.MultiheadAttention(embed_dim=embed_dim,\n",
    "                num_heads=num_heads,\n",
    "                dropout=dropout,\n",
    "                bias=bias)\n",
    "        self.act = nn.ReLU()\n",
    "        self.ln = nn.LayerNorm([max_len, embed_dim])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(1, 0, 2)\n",
    "        res, _ = self.attn(key=x, value=x, query=x)\n",
    "        res = self.act(res)\n",
    "        if self.skip_connection:\n",
    "            res = res + x\n",
    "        res = res.permute(1, 0, 2)\n",
    "        return self.ln(res)\n",
    "\n",
    "\n",
    "class Featurize(nn.Module):\n",
    "    def __init__(self, n_n, n_c, embed_dim, vocab_size, num_heads, dropout, bias=False):\n",
    "        '''\n",
    "        input: List of Tensors (num vars and cat vars))\n",
    "               num_vars: (bsz, len(num_vars))\n",
    "               token: (bsz, n_col)\n",
    "        output: featurize (bsz, 1+n_c, rep_dim)\n",
    "        '''\n",
    "        super(Featurize, self).__init__()\n",
    "        n_col = n_n + n_c\n",
    "        hidden_dim = (1+n_col) * embed_dim\n",
    "        \n",
    "        self.numeric_fc = FCUnit(input_dim=n_n,\n",
    "                output_dim=embed_dim,\n",
    "                dropout=dropout,\n",
    "                bias=bias)\n",
    "\n",
    "        self.embed = nn.Embedding(num_embeddings=vocab_size,\n",
    "                embedding_dim=embed_dim,\n",
    "                padding_idx=0)\n",
    "\n",
    "        self.attn = SelfAttentionUnit(embed_dim=embed_dim,\n",
    "                num_heads=num_heads,\n",
    "                max_len=n_col,\n",
    "                dropout=dropout,\n",
    "                bias=bias)\n",
    "        \n",
    "        self.apply(self._init_weight)\n",
    "    \n",
    "    def _init_weight(self, m):\n",
    "        if isinstance(m, nn.Embedding):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "    \n",
    "    def forward(self, x_num, tokens):\n",
    "        f_num = self.numeric_fc(x_num)\n",
    "        f_emb = self.embed(tokens)\n",
    "        f_int = self.attn(f_emb).flatten(start_dim=1)\n",
    "        f_tot = torch.cat([f_num, f_int], dim=1)\n",
    "        return f_tot\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, num_vars, cat_vars, tasks, rep_dim, vocab_size, num_heads, n_hidden, dropout):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        n_n = len(num_vars)\n",
    "        n_c = len(cat_vars)\n",
    "        n_col = n_n + n_c\n",
    "        hidden_dim = (1+n_col)*rep_dim\n",
    "\n",
    "        self.featurizer = Featurize(n_n=n_n,\n",
    "                n_c=n_c,\n",
    "                embed_dim=rep_dim,\n",
    "                vocab_size=vocab_size,\n",
    "                num_heads=num_heads,\n",
    "                dropout=dropout)\n",
    "        \n",
    "        shared_unit = [FCUnit(input_dim=hidden_dim,\n",
    "            output_dim=hidden_dim,\n",
    "            dropout=dropout)]\n",
    "\n",
    "        self.shared_unit = nn.Sequential(*shared_unit*n_hidden)\n",
    "\n",
    "        self.multi_task_mappings = nn.ModuleList()\n",
    "        self.multi_task_mappings.append(\n",
    "                nn.Linear(hidden_dim, n_n)\n",
    "        )\n",
    "\n",
    "        for t in tasks:\n",
    "            self.multi_task_mappings.append(\n",
    "                    nn.Linear(hidden_dim, t if t != 2 else 1)\n",
    "            )\n",
    "        \n",
    "        self.apply(self._init_weight)\n",
    "    \n",
    "    def _init_weight(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "    \n",
    "    def forward(self, x_num, tokens):\n",
    "        f_tot = self.featurizer(x_num, tokens)\n",
    "        z = self.shared_unit(f_tot)\n",
    "        outputs = []\n",
    "        for i, task in enumerate(self.multi_task_mappings):\n",
    "            output = task(z)\n",
    "            if i > 0 :\n",
    "                if output.shape[1] == 1:\n",
    "                    output = torch.sigmoid(output)\n",
    "                elif output.shape[1] > 1:\n",
    "                    output = torch.softmax(output, dim=-1)\n",
    "            outputs.append(output)\n",
    "        return outputs\n",
    "\n",
    "                \n",
    "class Imputer(object):\n",
    "\n",
    "    def __init__(self, rep_dim=32, num_heads=8, n_hidden=2,\n",
    "            lr=3e-3, weight_decay=1e-5, batch_size=128,\n",
    "            epochs=10, noise_percent=30, stopped_epoch=10):\n",
    "        \n",
    "        self.rep_dim = rep_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.stopped_epoch = stopped_epoch\n",
    "        self.dropout = 0.5\n",
    "        self.noise_percent = noise_percent\n",
    "\n",
    "    def cal_loss(self, preds, targets, masks):\n",
    "        losses = []\n",
    "        nums = []\n",
    "        eps = 1e-10\n",
    "        \n",
    "        loss_mse = F.mse_loss(preds[0], targets[0], reduction='none')\n",
    "        loss_mse *= masks[0]\n",
    "        N = masks[0].sum()\n",
    "        loss_mse = loss_mse.sum()/(N+eps)\n",
    "        losses.append(loss_mse)\n",
    "        nums.append(N)\n",
    "\n",
    "        for idx, t in enumerate(self.tasks, 1):\n",
    "            if t == 2: # bce\n",
    "                loss = F.binary_cross_entropy(input=preds[idx],\n",
    "                                              target=targets[idx].unsqueeze(-1).float(),\n",
    "                                              reduction='none').reshape(masks[idx].shape)\n",
    "            else: # cce\n",
    "                loss = F.cross_entropy(input=preds[idx],\n",
    "                                       target=targets[idx],\n",
    "                                       reduction='none').reshape(masks[idx].shape)\n",
    "            loss *= masks[idx]\n",
    "            N = masks[idx].sum()\n",
    "            loss = loss.sum()/(N+eps)\n",
    "            losses.append(loss)\n",
    "            nums.append(N)\n",
    "        \n",
    "        losses = torch.stack(losses, 0)\n",
    "        nums = torch.stack(nums, 0)\n",
    "        return losses, nums\n",
    "\n",
    "    def cal_metric_batch(self, preds, targets, masks):\n",
    "        with torch.no_grad():\n",
    "            metric = []\n",
    "\n",
    "            mse = (torch.pow(targets[0] - preds[0], 2) * masks[0]).sum() / masks[0].sum()\n",
    "            metric.append(mse.item())\n",
    "\n",
    "            for idx in range(1, len(preds)):\n",
    "                if preds[idx].shape[1] == 1:\n",
    "                    res = (preds[idx] > 0.5).to(torch.int).squeeze() == targets[idx]\n",
    "                else:\n",
    "                    res = torch.argmax(preds[idx], axis=1) == targets[idx]\n",
    "                acc = (res * masks[idx]).sum() / masks[idx].sum()\n",
    "                metric.append(acc.item())\n",
    "        return metric\n",
    "    \n",
    "    def run_batch(self, batch, epoch, train):\n",
    "        nums, tokens, targets, masks = batch\n",
    "        tokens = tokens.to(DEVICE)\n",
    "        nums = nums.to(DEVICE)\n",
    "        targets = [t.to(DEVICE) for t in targets]\n",
    "        masks = [m.to(DEVICE) for m in masks]\n",
    "        batch_size = targets[0].shape[0]\n",
    "\n",
    "        preds = self.model(nums, tokens)\n",
    "        losses, nums = self.cal_loss(preds, targets, masks)\n",
    "        metric = self.cal_metric_batch(preds, targets, masks)\n",
    "        \n",
    "        loss_tot = losses.mean()\n",
    "        \n",
    "        if train:\n",
    "            self.optimizer.zero_grad()\n",
    "            loss_tot.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "        metric = np.array(metric)\n",
    "        losses = np.array([l.item() for l in losses])\n",
    "        nums = np.array([n.item() for n in nums])\n",
    "        \n",
    "        return loss_tot.item(), metric, losses, nums, batch_size\n",
    "\n",
    "    def train(self, epoch=None):\n",
    "        self.model.train()\n",
    "        t_loss_tot, cnt = 0., 0.\n",
    "        n_task = len(self.tasks)+1\n",
    "        t_metrics = np.zeros(n_task)\n",
    "        t_losses = np.zeros(n_task)\n",
    "        mask_cnt = np.zeros(n_task)\n",
    "\n",
    "        for batch_idx, batch in enumerate(self.train_iter):\n",
    "            loss_tot, metric, losses, nums, batch_size = self.run_batch(batch, epoch, train=True)\n",
    "            t_metrics += metric * nums\n",
    "            t_loss_tot += loss_tot * batch_size\n",
    "            cnt += batch_size\n",
    "            t_losses += losses * nums\n",
    "            mask_cnt += nums\n",
    "            \n",
    "        status = {\n",
    "            'task_metrics' : t_metrics/mask_cnt,\n",
    "            'task_losses' : t_losses/mask_cnt,\n",
    "            'total_loss' : t_loss_tot/cnt\n",
    "        }\n",
    "        \n",
    "        return status\n",
    "    \n",
    "    def fit(self, X, cat_vars=None):\n",
    "        \n",
    "        self.n_col = X.shape[1]\n",
    "        self.cat_vars = cat_vars if cat_vars else list()\n",
    "        num_vars = set(range(self.n_col)) - set(self.cat_vars)\n",
    "        num_vars = list(num_vars)\n",
    "        self.num_vars = num_vars\n",
    "        \n",
    "        self.c2is = None\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(X[:, self.num_vars])\n",
    "        mapper = Mapper(X=X, num_vars=num_vars, cat_vars=cat_vars)\n",
    "        tasks = [int(np.nan_to_num(X[:, idx].astype('float')).max()+1) for idx in cat_vars] \n",
    "        \n",
    "        self.scaler = scaler\n",
    "        self.mapper = mapper\n",
    "        self.tasks = tasks\n",
    "\n",
    "        self.model = Model(num_vars=num_vars,\n",
    "                cat_vars=cat_vars,\n",
    "                tasks=tasks,\n",
    "                rep_dim=self.rep_dim,\n",
    "                vocab_size=len(mapper.f2i),\n",
    "                num_heads=self.num_heads,\n",
    "                n_hidden=self.n_hidden,\n",
    "                dropout=self.dropout).to(DEVICE)\n",
    "       \n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(),\n",
    "                                          lr=self.lr,\n",
    "                                          weight_decay=self.weight_decay)\n",
    "\n",
    "        dataset = MVIDataset(X, cat_vars, num_vars, scaler, mapper, self.noise_percent)\n",
    "        self.train_iter = DataLoader(dataset=dataset,\n",
    "                                     batch_size=self.batch_size,\n",
    "                                     shuffle=True,\n",
    "                                     num_workers=0)\n",
    "    \n",
    "        wait = 0\n",
    "        best_loss = None\n",
    "        iteration = range(1, self.epochs+1)\n",
    "        \n",
    "        for epoch in iteration:\n",
    "            epoch_status = self.train(epoch)\n",
    "            \n",
    "            if self.stopped_epoch:\n",
    "                if best_loss is None or best_loss > epoch_status['total_loss']:\n",
    "                    best_loss = epoch_status['total_loss']\n",
    "                    wait = 0\n",
    "                else:\n",
    "                    wait += 1\n",
    "                \n",
    "                if wait > self.stopped_epoch:\n",
    "                    print('Terminated Training for Early Stopping at Epoch {}'.format(epoch))\n",
    "                    break\n",
    "            \n",
    "        return self\n",
    "\n",
    "    def np_inverse_transform(self, array, dic):\n",
    "        u, inv = np.unique(array, return_inverse=True)\n",
    "        return np.array([dic[x] for x in u])[inv].reshape(array.shape)\n",
    "    \n",
    "    def _transform(self, outputs):\n",
    "        batch_size = outputs[0].shape[0]\n",
    "        rec_num_vars = self.scaler.inverse_transform(outputs[0].cpu().detach().numpy())\n",
    "        rec_cat_vars = []\n",
    "        for output in outputs[1:]:\n",
    "            if output.shape[1] == 1:\n",
    "                output = (output.cpu().detach().numpy() > 0.5).astype('int')\n",
    "            else:\n",
    "                output = np.argmax(output.cpu().detach().numpy(), axis=1).reshape(-1, 1)\n",
    "            rec_cat_vars.append(output.astype('object'))\n",
    "        if len(rec_cat_vars) > 0:\n",
    "            rec_cat_vars = np.concatenate(rec_cat_vars, axis=1)\n",
    "\n",
    "        result = np.zeros((batch_size, self.n_col))\n",
    "        result[:, self.num_vars] = rec_num_vars\n",
    "        result = result.astype('object')\n",
    "        result[:, self.cat_vars] = rec_cat_vars\n",
    "        return result\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \n",
    "        dataset = MVIDataset(X, self.cat_vars, self.num_vars, self.scaler, self.mapper)\n",
    "        impute_iter = DataLoader(dataset=dataset,\n",
    "                                 batch_size = self.batch_size,\n",
    "                                 shuffle=False,\n",
    "                                 num_workers=0)\n",
    "        \n",
    "        X_imputed = []\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, batch in enumerate(impute_iter):\n",
    "                nums, tokens, _, _ = batch\n",
    "                nums = nums.to(DEVICE)\n",
    "                tokens = tokens.to(DEVICE)\n",
    "                \n",
    "                outputs = self.model(nums, tokens)\n",
    "                result = self._transform(outputs)\n",
    "                X_imputed.append(result)\n",
    "        \n",
    "        return np.vstack(X_imputed)\n",
    "\n",
    "    def fit_transform(self, X, **fit_params):\n",
    "        \n",
    "        return self.fit(X, **fit_params).transform(X)\n",
    "\n",
    "\n",
    "class MVImputer(object):\n",
    "\n",
    "    def __init__(self, input_fname, result_path, result_csv_path):\n",
    "        self.label_encoders = defaultdict(LabelEncoder)\n",
    "        self.get_data(input_fname)\n",
    "        self.input_fname = input_fname\n",
    "        self.result_path = result_path\n",
    "        self.result_csv_path = result_csv_path\n",
    "\n",
    "    def get_data(self, input_fname):\n",
    "        data = pd.read_csv(input_fname)\n",
    "        na_loc = data.isnull()\n",
    "        for idx in data.columns:\n",
    "            if idx.startswith('cat'):\n",
    "                data[idx] = data[idx].astype(str)\n",
    "        num_idx = data.dtypes[data.dtypes != 'object'].index\n",
    "        num_vars = [data.columns.get_loc(idx) for idx in num_idx]\n",
    "        cat_vars = list(set(range(data.shape[1])) - set(num_vars))\n",
    "        data[na_loc] = np.nan\n",
    "        \n",
    "        self.data = data\n",
    "        self.num_vars = num_vars\n",
    "        self.cat_vars = cat_vars\n",
    "    \n",
    "    def label_encode(self, data):\n",
    "        missing = pd.isnull(data.iloc[:, self.cat_vars])\n",
    "        encode_values = data.iloc[:, self.cat_vars].fillna('NULL')\n",
    "        encode_values = encode_values.apply(\n",
    "                lambda x: self.label_encoders[x.name].fit_transform(x)\n",
    "                )\n",
    "        encode_values = encode_values.values.astype('float')\n",
    "        encode_values[missing] = np.nan\n",
    "        data.iloc[:, self.cat_vars] = encode_values\n",
    "        return data\n",
    "\n",
    "    def inverse_transform(self, data):\n",
    "        inverse_values = data.iloc[:, self.cat_vars].astype('int')\n",
    "        inverse_values = inverse_values.apply(\n",
    "                lambda x: self.label_encoders[x.name].inverse_transform(x)\n",
    "                ) \n",
    "        data.iloc[:, self.cat_vars] = inverse_values.values\n",
    "        return data\n",
    "\n",
    "    def run(self):\n",
    "\n",
    "        if not os.path.isdir(self.result_path):\n",
    "            os.makedirs(self.result_path)\n",
    "\n",
    "        data = self.label_encode(self.data)\n",
    "        X_incomplete = data.values\n",
    "        targets = pd.isnull(X_incomplete)\n",
    "        \n",
    "        imputer = Imputer()\n",
    "        X_imputed = imputer.fit_transform(X_incomplete, cat_vars=self.cat_vars)\n",
    "        imputed_values = X_incomplete.copy()\n",
    "        imputed_values[targets] = X_imputed[targets]\n",
    "        result_data = pd.DataFrame(imputed_values, columns=data.columns)\n",
    "        result_data = self.inverse_transform(result_data)\n",
    "        result_data.to_csv(self.result_csv_path, index=False)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    missing_mechanism = ['MCAR','MAR','MNAR']\n",
    "    missing_ratio = ['miss10','miss30','miss50']\n",
    "    casenum = 14\n",
    "    for m_m in tqdm(missing_mechanism):\n",
    "        for m_r in tqdm(missing_ratio):\n",
    "            folder_path = './data_stored/data_dsan/{}/Case{}/{}'.format(m_m, casenum, m_r)\n",
    "            # 10 sample missing dataset\n",
    "            for smp in tqdm(range(10)):\n",
    "                i_fname = './data_stored/data_miss/{}/Case{}/{}/{}.csv'.format(m_m, casenum, m_r, smp)\n",
    "                res_csv_path = './data_stored/data_dsan/{}/Case{}/{}/{}.csv'.format(m_m, casenum, m_r, smp)\n",
    "    \n",
    "                mvi = MVImputer(input_fname = i_fname, result_path = folder_path, result_csv_path = res_csv_path)\n",
    "                mvi.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03415bae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
